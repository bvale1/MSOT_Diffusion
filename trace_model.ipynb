{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd9db986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in ./venv/lib/python3.12/site-packages (1.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvista\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import denoising_diffusion_pytorch as ddp\n",
    "from functools import partial\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "from edm2.training.networks_edm2 import Precond\n",
    "\n",
    "import end_to_end_phantom_QPAT.utils.networks as e2eQPAT_networks\n",
    "import utility_functions as uf\n",
    "from epoch_steps import *\n",
    "from nn_modules.time_conditioned_residual_unet import TimeConditionedResUNet\n",
    "from nn_modules.DiT import DiT\n",
    "from nn_modules.swin_unet import SwinTransformerSys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60cbb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments (from uf.get_config() in run_model.py)\n",
    "model_name = 'UNet_e2eQPAT'\n",
    "image_size = 288\n",
    "channels = 1\n",
    "predict_fluence = True\n",
    "attention = False\n",
    "use_torchsummary = True\n",
    "use_torchvista = False # torchvista currently does not support some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31f4e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "RegressionUNet                           [8, 2, 288, 288]          --\n",
      "├─Sequential: 1-1                        [8, 64, 288, 288]         --\n",
      "│    └─Conv2d: 2-1                       [8, 64, 288, 288]         640\n",
      "│    └─LeakyReLU: 2-2                    [8, 64, 288, 288]         --\n",
      "│    └─Conv2d: 2-3                       [8, 64, 288, 288]         36,928\n",
      "│    └─LeakyReLU: 2-4                    [8, 64, 288, 288]         --\n",
      "├─Sequential: 1-2                        [8, 64, 144, 144]         --\n",
      "│    └─Conv2d: 2-5                       [8, 64, 144, 144]         36,928\n",
      "│    └─LeakyReLU: 2-6                    [8, 64, 144, 144]         --\n",
      "├─Sequential: 1-3                        [8, 128, 144, 144]        --\n",
      "│    └─Conv2d: 2-7                       [8, 128, 144, 144]        73,856\n",
      "│    └─LeakyReLU: 2-8                    [8, 128, 144, 144]        --\n",
      "│    └─Conv2d: 2-9                       [8, 128, 144, 144]        147,584\n",
      "│    └─LeakyReLU: 2-10                   [8, 128, 144, 144]        --\n",
      "├─Sequential: 1-4                        [8, 128, 72, 72]          --\n",
      "│    └─Conv2d: 2-11                      [8, 128, 72, 72]          147,584\n",
      "│    └─LeakyReLU: 2-12                   [8, 128, 72, 72]          --\n",
      "├─Sequential: 1-5                        [8, 256, 72, 72]          --\n",
      "│    └─Conv2d: 2-13                      [8, 256, 72, 72]          295,168\n",
      "│    └─LeakyReLU: 2-14                   [8, 256, 72, 72]          --\n",
      "│    └─Conv2d: 2-15                      [8, 256, 72, 72]          590,080\n",
      "│    └─LeakyReLU: 2-16                   [8, 256, 72, 72]          --\n",
      "├─Sequential: 1-6                        [8, 256, 36, 36]          --\n",
      "│    └─Conv2d: 2-17                      [8, 256, 36, 36]          590,080\n",
      "│    └─LeakyReLU: 2-18                   [8, 256, 36, 36]          --\n",
      "├─Sequential: 1-7                        [8, 512, 36, 36]          --\n",
      "│    └─Conv2d: 2-19                      [8, 512, 36, 36]          1,180,160\n",
      "│    └─LeakyReLU: 2-20                   [8, 512, 36, 36]          --\n",
      "│    └─Conv2d: 2-21                      [8, 512, 36, 36]          2,359,808\n",
      "│    └─LeakyReLU: 2-22                   [8, 512, 36, 36]          --\n",
      "├─Sequential: 1-8                        [8, 512, 18, 18]          --\n",
      "│    └─Conv2d: 2-23                      [8, 512, 18, 18]          2,359,808\n",
      "│    └─LeakyReLU: 2-24                   [8, 512, 18, 18]          --\n",
      "├─Sequential: 1-9                        [8, 512, 36, 36]          --\n",
      "│    └─Conv2d: 2-25                      [8, 1024, 18, 18]         4,719,616\n",
      "│    └─LeakyReLU: 2-26                   [8, 1024, 18, 18]         --\n",
      "│    └─Conv2d: 2-27                      [8, 1024, 18, 18]         9,438,208\n",
      "│    └─LeakyReLU: 2-28                   [8, 1024, 18, 18]         --\n",
      "│    └─ConvTranspose2d: 2-29             [8, 512, 36, 36]          2,097,664\n",
      "│    └─LeakyReLU: 2-30                   [8, 512, 36, 36]          --\n",
      "├─Sequential: 1-10                       [8, 512, 36, 36]          --\n",
      "│    └─Conv2d: 2-31                      [8, 512, 36, 36]          4,719,104\n",
      "│    └─LeakyReLU: 2-32                   [8, 512, 36, 36]          --\n",
      "├─Sequential: 1-11                       [8, 512, 36, 36]          --\n",
      "│    └─Conv2d: 2-33                      [8, 512, 36, 36]          2,359,808\n",
      "│    └─LeakyReLU: 2-34                   [8, 512, 36, 36]          --\n",
      "├─ConvTranspose2d: 1-12                  [8, 256, 72, 72]          524,544\n",
      "├─Sequential: 1-13                       [8, 256, 72, 72]          --\n",
      "│    └─Conv2d: 2-35                      [8, 256, 72, 72]          1,179,904\n",
      "│    └─LeakyReLU: 2-36                   [8, 256, 72, 72]          --\n",
      "├─Sequential: 1-14                       [8, 256, 72, 72]          --\n",
      "│    └─Conv2d: 2-37                      [8, 256, 72, 72]          590,080\n",
      "│    └─LeakyReLU: 2-38                   [8, 256, 72, 72]          --\n",
      "├─ConvTranspose2d: 1-15                  [8, 128, 144, 144]        131,200\n",
      "├─Sequential: 1-16                       [8, 128, 144, 144]        --\n",
      "│    └─Conv2d: 2-39                      [8, 128, 144, 144]        295,040\n",
      "│    └─LeakyReLU: 2-40                   [8, 128, 144, 144]        --\n",
      "├─Sequential: 1-17                       [8, 128, 144, 144]        --\n",
      "│    └─Conv2d: 2-41                      [8, 128, 144, 144]        147,584\n",
      "│    └─LeakyReLU: 2-42                   [8, 128, 144, 144]        --\n",
      "├─ConvTranspose2d: 1-18                  [8, 64, 288, 288]         32,832\n",
      "├─Sequential: 1-19                       [8, 64, 288, 288]         --\n",
      "│    └─Conv2d: 2-43                      [8, 64, 288, 288]         73,792\n",
      "│    └─LeakyReLU: 2-44                   [8, 64, 288, 288]         --\n",
      "├─Sequential: 1-20                       [8, 64, 288, 288]         --\n",
      "│    └─Conv2d: 2-45                      [8, 64, 288, 288]         36,928\n",
      "│    └─LeakyReLU: 2-46                   [8, 64, 288, 288]         --\n",
      "├─Conv2d: 1-21                           [8, 2, 288, 288]          130\n",
      "==========================================================================================\n",
      "Total params: 34,165,058\n",
      "Trainable params: 34,165,058\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 577.09\n",
      "==========================================================================================\n",
      "Input size (MB): 2.65\n",
      "Forward/backward pass size (MB): 3397.39\n",
      "Params size (MB): 136.66\n",
      "Estimated Total Size (MB): 3536.70\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S')\n",
    "device = torch.device('cpu')\n",
    "logging.info(f'using device: {device}')\n",
    "\n",
    "# ==================== Data ====================\n",
    "\n",
    "match model_name:\n",
    "    case 'UNet_e2eQPAT' | 'UNet_wl_pos_emb' | 'UNet_diffusion_ablation' | 'Swin_UNet':\n",
    "        example_input = torch.ones((8, 1, image_size, image_size))\n",
    "    case 'DDIM' | 'DiT':\n",
    "        raise NotImplementedError\n",
    "    case 'EDM2':\n",
    "        example_input = torch.ones((8, 2, image_size, image_size))\n",
    "        x_cond = torch.ones((8, 1, image_size, image_size))\n",
    "        t = torch.ones((8, 1))\n",
    "        wavelengths_one_hot = torch.ones((8, 1000))\n",
    "\n",
    "# ==================== Model ====================\n",
    "channels = 1\n",
    "out_channels = channels * 2 if predict_fluence else channels\n",
    "match model_name:\n",
    "    case 'UNet_e2eQPAT':\n",
    "        model = e2eQPAT_networks.RegressionUNet(\n",
    "            in_channels=channels, out_channels=out_channels,\n",
    "            initial_filter_size=64, kernel_size=3\n",
    "        )\n",
    "        if use_torchsummary:\n",
    "            summary(model, input_size=(8, channels, image_size, image_size), device='cpu', verbose=1)\n",
    "        if use_torchvista:\n",
    "            torchvista.trace_model(model, example_input)\n",
    "    case 'UNet_wl_pos_emb' | 'UNet_diffusion_ablation':\n",
    "        model = ddp.Unet(\n",
    "            dim=32, channels=channels, out_dim=out_channels,\n",
    "            self_condition=False, image_condition=False, use_attn=attention,\n",
    "            full_attn=False, flash_attn=False, learned_sinusoidal_cond=False, \n",
    "        )\n",
    "        #model = TimeConditionedResUNet(\n",
    "        #    dim_in=channels, dim_out=out_channels, dim_first_layer=64,\n",
    "        #    kernel_size=3, theta_pos_emb=10000, self_condition=False,\n",
    "        #    image_condition=False\n",
    "        #)\n",
    "    case 'Swin_UNet':\n",
    "        model = SwinTransformerSys(\n",
    "            img_size=image_size[0], patch_size=4, in_chans=channels, num_classes=out_channels,\n",
    "            embed_dim=96, depths=[2, 2, 2, 2], depths_decoder=[1, 2, 2, 2], num_heads=[3, 6, 12, 24],\n",
    "            window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "            drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "            norm_layer=nn.LayerNorm, ape=False, patch_norm=False,\n",
    "            final_upsample=\"expand_first\"\n",
    "        )\n",
    "        uf.remove_softmax(model)\n",
    "    case 'DDIM':\n",
    "        model = ddp.Unet(\n",
    "            dim=32, channels=out_channels, out_dim=out_channels,\n",
    "            self_condition=True, image_condition=True, \n",
    "            image_condition_channels=channels, use_attn=attention,\n",
    "            full_attn=False, flash_attn=False\n",
    "        )\n",
    "        #model = TimeConditionedResUNet(\n",
    "        #    dim_in=out_channels, dim_out=out_channels, dim_first_layer=64,\n",
    "        #    kernel_size=3, theta_pos_emb=10000, self_condition=args.self_condition,\n",
    "        #    image_condition=True, dim_image_condition=channels\n",
    "        #)\n",
    "        diffusion = ddp.GaussianDiffusion(\n",
    "            # objecive='pred_v' predicts the velocity field, objective='pred_noise' predicts the noise\n",
    "            model, image_size=image_size, timesteps=1000,\n",
    "            sampling_timesteps=100, objective='pred_v', auto_normalize=False,\n",
    "        )\n",
    "    case 'DiT':\n",
    "        # parameters depth=12, hidden_size=384, and num_heads=6 are the same as DiT-S/8.\n",
    "        # with an image size of 256 and patch size of 16, we have the \n",
    "        # same number of patches as ViT from an image is worth 16x16 words\n",
    "        #if image_size[0] % 16 != 0:\n",
    "        #    raise ValueError('image size must be divisible by 16 for DiT model')\n",
    "        #patch_size = image_size[0] // 16\n",
    "        patch_size = 4\n",
    "        model = DiT(\n",
    "            dim_in=out_channels, dim_out=out_channels, input_size=image_size, \n",
    "            depth=12, hidden_size=384, patch_size=patch_size, num_heads=6,\n",
    "            self_condition=True, image_condition=True\n",
    "        )\n",
    "        diffusion = ddp.GaussianDiffusion(\n",
    "            # objecive='pred_v' predicts the velocity field, objective='pred_noise' predicts the noise\n",
    "            model, image_size=image_size, timesteps=1000,\n",
    "            sampling_timesteps=100, objective='pred_v', auto_normalize=False,\n",
    "        )\n",
    "    case 'EDM2':\n",
    "        attn_resolutions = [16, 8] if attention else []\n",
    "        in_channels = out_channels+1 # plus 1 for conditional information\n",
    "        loss_fn = EDM2Loss(P_mean=-0.8, P_std=1.6, sigma_data=0.5)\n",
    "        model = Precond(\n",
    "            img_resolution=image_size, img_channels_in=in_channels, img_channels_out=out_channels, #img_channels_in=in_channels, img_channels_out=out_channels,\n",
    "            label_dim=1000, model_channels=64, attn_resolutions=attn_resolutions, \n",
    "            use_fp16=False, sigma_data=0.5\n",
    "        )\n",
    "        model.unet.forward = partial(model.unet.forward, noise_labels=t.flatten(), class_labels=wavelengths_one_hot)\n",
    "        if use_torchsummary:\n",
    "            summary(model.unet, input_size=((8, in_channels, image_size, image_size)), device='cpu', verbose=1)\n",
    "        if use_torchvista:\n",
    "            torchvista.trace_model(model.unet, torch.cat([example_input, x_cond], dim=1), collapse_modules_after_depth=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
